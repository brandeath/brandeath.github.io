<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Blog &#8211; Yanran's Attic</title>
<meta name="description" content="Natural Language Processing, Machine Learning, Deep Learning, R">
<meta name="keywords" content="Natural Language Processing, Machine Learning, Deep Learning, Text Mining, R, Theano, GPU">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Blog">
<meta property="og:description" content="Natural Language Processing, Machine Learning, Deep Learning, R">
<meta property="og:url" content="/page2/index.html">
<meta property="og:site_name" content="Yanran's Attic">





<link rel="canonical" href="/page2/">
<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Yanran's Attic Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.min.css">
<!-- Webfonts -->
<link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="/images/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="/images/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="/images/favicon.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="/images/favicon.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="/images/favicon.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/images/favicon.png">
<link rel="shortcut icon" href="/images/favicon.ico"/>
<link rel="bookmark" href="/images/favicon.ico"/>



<!-- MathJax -->
<script type="text/javascript"
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




<script type="text/javascript"
src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

<body id="post-index" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="/images/avatar.jpg" alt="Yanran Li photo" class="author-photo">
					<h4>Yanran Li</h4>
					<p>Research Assistant at PolyU</p>
				</li>
				<li><a href="/about/">Learn More</a></li>
				<li>
					<a href="mailto:yanranli.summer@gmail.com"><i class="icon-envelope"></i> Email</a>
				</li>
				
				
				<li>
					<a href="http://weibo.com/summerrlee"><i class="icon-weibo"></i> Weibo</a>
				</li>
				
				<li>
					<a href="https://www.linkedin.com/profile/view?id=233043238"><i class="icon-linkedin"></i> LinkedIn</a>
				</li>
				<li>
					<a href="http://github.com/niangaotuantuan"><i class="icon-github-alt"></i> GitHub</a>
				</li>
				
				
				
				
<!-- 				 -->
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="/posts/">All Posts</a></li>
				<li><a href="/categories/">All Categories</a></li>			
				<li><a href="/tags/">All Tags</a></li>			
			</ul>
		</li>
		<li><a href="/categories/#peppypapers">PaperNotes</a></li><li><a href="/guestbook">GuestBook</a></li><li><a href="/friends">Friends</a></li><li><a href="/resources">Publications of Deep Learning in NLP</a></li><li><a href="https://web.cs.dal.ca/~vlado/nlp/" class="external">NLP links</a></li><li><a href="http://www.reddit.com/r/machinelearning" class="external">r/machinelearning</a></li><li><a href="https://plus.google.com/communities/112866381580457264725" class="external">Deep Learning G+</a></li>
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  
  
    <div class="entry-image">
      <img src="/images/bg_main.jpg" alt="Blog">
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>Yanran's Attic</h1>
      <h2>Blog</h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main">
  
<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2014-07-07T00:00:00+08:00"><a href="/machinelearning/2014/07/07/ADMM-%E6%A1%86%E6%9E%B6%E5%B0%8F%E7%BB%93.html">July 07, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="/machinelearning/2014/07/07/ADMM-%E6%A1%86%E6%9E%B6%E5%B0%8F%E7%BB%93.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="/machinelearning/2014/07/07/ADMM-%E6%A1%86%E6%9E%B6%E5%B0%8F%E7%BB%93.html" rel="bookmark" title="ADMM 框架小结" itemprop="url">ADMM 框架小结</a></h1>
    
  </header>
  <div class="entry-content">
    <p>ADMM 是这两年很火的一个算法框架。</p>

<p>其主要的提出想法是为了“分布式”的解决一类问题，即：
<script type="math/tex">min f(x) + g(y)
s.t. Ax + By = c</script>
是等式约束（不可以是不等式）的最小化加和问题；且此时 f(x) 和 g(y) 是两类性质非常不同的函数，比如 LASSO 中的 1-norm 惩罚和前面的代价函数。</p>

<p>对于这个问题的 augmentated Lagrangian:</p>

<script type="math/tex; mode=display">L_{\rho}(x,z,y)=f(x)+g(z)+y^T(Ax+Bz-c)+(\rho/2)\|Ax+Bz-c\|_2^2</script>

<p>和对应的 update rule:</p>

<script type="math/tex; mode=display">x^{k+1}=\arg\min_x L_{\rho}(x,z^k,y^k)</script>

<script type="math/tex; mode=display">z^{k+1}=\arg\min_z L_{\rho}(x^{k+1},z,y^k)</script>

<script type="math/tex; mode=display">y^{k+1}=y^k+\rho(Ax^{k+1}+Bz^{k+1}-c)</script>

<p>可以看出，上图中的第一个式子和第二个式子是乘子法的一个特殊化；且第一个式子对应的是乘子法中的”x-minimization step”，第二个式子是所谓的“dual update”。</p>

<p>不同的是，乘子法是同时计算并最小化x,z：
<script type="math/tex">(x^{k+1},z^{k+1})=\arg\min_{x,z} L_{\rho}(x,z,y^k)</script></p>

<p>而ADMM中，x,z 是“alternated”计算的，它 decouples x-min step from y-min step。</p>

<p>这里可以看到，augmentated Lagrangian 虽然弱化了乘子法的强假设性，但 x-min step 引入了二次项而导致无法把 x 分开进行求解。所以 ADMM 也是就是期望结合乘子法的弱条件的收敛性以及对偶上升法的可分解求解性。</p>

<p>其实 ADMM 也是一个需要 trick 的框架：</p>

<ol>
  <li>
    <p>因为它的缺点是需要非常多步的迭代才能得到相对精确的解，这就好像是一阶算法。</p>
  </li>
  <li>
    <p>另外，对 $\rho$ 的选择也很重要，非常影响收敛性；$\rho$ 太大，对于 min (f1+f2) 就不够重视；反之，则对于 feasibility 又不够重视。Boyd et al. (2010) 倒是给了实践中改变 $\rho$ 的策略，可是也没有证明它的收敛性。</p>
  </li>
</ol>

<p>Boyd 在其<a href="http://web.stanford.edu/~boyd/papers/admm/">网站</a>上给出了一些例子。总结这里的几个例子，构造 ADMM 的形式，主要思想就是往直前的受约束的凸优化问题靠拢。
（1）对于只有传统损失函数没有正则项的（比如LAD, Huber Fitting），构造出一个 z。
（2）对于有约束的，比如 l1-norm 的约束，则把约束变成 g(x)，原始损失函数为 f(x)。若 f(x) 本身没有（比如Basic Pursuit），就构造成带有定义域约束的 f(x)（某种投影）；如果有，则就是比较一般化的损失+正则问题，这时候就基本是一个<script type="math/tex">f(x)+\lambda\|z\|_1</script>的形式。是非常自然的 ADMM。</p>

<p>所以，像广义线性模型和广义可加模型+正则等等都非常适合 ADMM。</p>

<p><em>Ref:</em></p>

<p>[1] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers, 2010.</p>

<p>[2] S. Boyd. Alternating Direction Method of Multipliers (Slides)</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2013-11-11T00:00:00+08:00"><a href="/naturallanguageprocessing/2013/11/11/Relation-Extraction-with-Matrix-Factorization.html">November 11, 2013</a></time></span><span class="author vcard"><span class="fn"><a href="/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="/naturallanguageprocessing/2013/11/11/Relation-Extraction-with-Matrix-Factorization.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="/naturallanguageprocessing/2013/11/11/Relation-Extraction-with-Matrix-Factorization.html" rel="bookmark" title="Relation Extraction with Matrix Factorization" itemprop="url">Relation Extraction with Matrix Factorization</a></h1>
    
  </header>
  <div class="entry-content">
    <p>This post is about the NAACL’13 Accepted Paper, <strong>Relation Extraction with Matrix Factorization and Universal Schemas</strong>. The talk is available on <a href="http://techtalks.tv/talks/relation-extraction-with-matrix-factorization-and-universal-schemas/58435/">techtalks</a>.</p>

<p>And then present some basic knowledge of <strong>Matrix Factorization</strong>.</p>

<h2 id="abstract">Abstract</h2>

<p>The paper studies techniques for inferring a model of entities and relations capable of performing basic types of semantic inference (e.g., predicting if a specific relation holds for a given pair of entities). The models exploit different types of embeddings of entities and relations.</p>

<p>This problem is usually tackled either via distant weak supervision from a knowledge base (providing structure and relational schemas) or in a totally unsupervised fashion (without any pre-defined schemas). The present approach aims at combining both trends with the introduction of universal schemas that can blend pre-defined ones from knowledge bases and uncertain ones extracted from free text.  This paper is very ambitious and interesting.</p>

<h2 id="related-work">Related Work</h2>

<h3 id="relation-extraction">relation extraction</h3>

<p>There has been a lot of previous research on learning entailment (aka inference) rules (e.g., Chkolvsky and Pantel 2004; Berant et al, ACL 2011; Nakashole et al, ACL 2012). 
Also, there has been some of the very related work on embedding relations, e.g., Bordes et al (AAAI 2011), or, very closely related, Jenatton et al (NIPS 2012).</p>

<h3 id="matrix-factorization">Matrix Factorization</h3>

<p><strong>Matrix factorization</strong> as a technique of <em>Collaborative filtering</em> has been the
preferred choice for recommendation systems ever since Netflix million competition was held a few years back. Further, with the advent of news personalization, advanced search and user analytics, the concept has gained
prominence.</p>

<p>In this paper, columns correspond to relations, and rows correspond to entity tuples. By contrast, in (Murphy et al., 2012) columns are words, and rows
are contextual features such as “words in a local window.” Consequently, this paper’s objective is to complete the matrix, whereas their objective is to learn better latent embeddings of words (which by themselves again cannot capture any sense of asymmetry).</p>

<h2 id="save-storage">Save Storage</h2>

<p>Although the paper doesn’t explicit point out how common is it that a tuple shares many relations, it remains concern. The experiments seem to show that mixing data sources is beneficial.</p>

<h2 id="trends">Trends</h2>

<p>The researchers are ambitious to bridge knowledges bases and text for information extraction, and this paper seems to go along this trend.
However, the paper’s scheme is limited before complex named entity disambiguation is solved, since it relies on the fact that entities constituting tuples from the Freebase and tuples extracted from the text have been exactly matched beforehand.</p>

<h2 id="generalized-matrix-factorization">Generalized Matrix Factorization</h2>

<p>It has been a general machine learning problem formulated as:</p>

<h3 id="training-data">Training data</h3>
<ul>
  <li><strong>V</strong>: m x n input matrix (e.g., rating matrix)</li>
  <li>Z: training set of indexes in <strong>V</strong> (e.g., subset of known ratings)</li>
</ul>

<h3 id="parameter-space">Parameter space</h3>
<ul>
  <li><strong>W</strong>: row factors (e.g., m x r latent customer factors)</li>
  <li><strong>H</strong>: column factors (e.g., r x n latent movie factors)</li>
</ul>

<h3 id="model">Model</h3>
<ul>
  <li><script type="math/tex">L_{ij}(W_{i*},H_{*j})</script>: loss at element (<em>i</em>,<em>j</em>)</li>
  <li>Includes prediction error, regularization, auxiliary information, . . .</li>
  <li>Constraints (e.g., non-negativity)</li>
</ul>

<h3 id="find-best-model">Find best model</h3>

<script type="math/tex; mode=display">\arg\min_{W,H}\sum_{(i,j)\in Z}L_{i,j}(W_{i*},H_{*j})</script>

<h2 id="stochastic-gradient-descent-for-matrix-factorization">Stochastic Gradient Descent for Matrix Factorization</h2>

<p>Among the various algorithmic techniques available, the following are more
popular: <strong>Alternating Least Squares (ALS)</strong>， <strong>Non-Negative Matrix Factorization</strong> and <strong>Stochastic Gradient Descent (SGD)</strong>. Here I only presents SGD for MF.</p>

<p><strong>SDG</strong> is a well know technique which tends to compute direction of steepest descent and then takes a step in that direction. Among the variants include:</p>

<p>(a)Partitioned SGD: distribute without using stratification and run independently and in parallel on partitions (b)Pipelined SGD: based on ‘delayed update’ scheme (c)Decentralized SGD: computation in decentralized and distributed fashion</p>

<p>The main solution is as follows:</p>

<ul>
  <li>
    <p>Set <script type="math/tex">\theta = (W,H)</script> and use</p>

    <p><script type="math/tex">L(\theta)=\sum_{(i,j)\in Z}L_{ij}(W_{i*},H_{*j})</script>,  <br />
  <script type="math/tex">{L}'(\theta)=\sum_{(i,j)\in Z}{L}'_{ij}(W_{i*},H_{*j})</script>,  <br />
  <script type="math/tex">{\hat{L}}'(\theta,z)=N{L}'_{i_{z}j_{z}}(W_{i_{z}*},H_{*j_{z}})</script>, where <script type="math/tex">N=\vert Z\vert</script></p>
  </li>
  <li>
    <p>SGD epoch</p>
    <ul>
      <li>Pick a random entry <script type="math/tex">z \in Z</script></li>
      <li>Compute approximate gradient <script type="math/tex">{\hat{L}}'(\theta,z)</script></li>
      <li>Update parameters <script type="math/tex">\theta_{n+1}=\theta_{n}-\epsilon_{n}{\hat{L}}'(\theta,z)</script></li>
      <li>Repeat <script type="math/tex">N</script> times</li>
    </ul>
  </li>
</ul>

<h2 id="svm-vs-fm">SVM V.S. FM</h2>

<p><strong>FM</strong> is short for <a href="http://www.libfm.org/"><strong>Factorization Machine</strong></a>. Indeed, it can be interpreted as <strong>Factorization</strong> Methods and Support Vector <strong>Machine</strong>. It is firstly published by Steffen Rendle.</p>

<p>Factorization machines (FM) are a generic approach that allows to mimic most factorization models by feature engineering. This way, factorization machines combine the generality of feature engineering with the superiority of factorization models in estimating interactions between categorical variables of large domain. libFM is a software implementation for factorization machines that features stochastic gradient descent (SGD) and alternating least squares (ALS) optimization as well as Bayesian inference using Markov Chain Monte Carlo (MCMC).</p>

<p><img src="http://i.imgur.com/Kc7q9Pl.png" alt="" /></p>

<p>in SVM mode, <script type="math/tex">y(x)=w\cdot x+b=w_{u}+w_{i}+...+b=\sum w_{i}x_{i}+b</script>, but original SVM fails with 2 main problems using here: <em>Real Value V.S. Classification</em>, and <em>Sparsity</em>.</p>

<p>in Factorization Machine mode, it is solved as: <script type="math/tex">y(x)=\sum w_{i}x_{i}+\sum\sum(v_{i}\cdot v_{j})x_{i}x_{j} +b</script>. The second part in the formula is <strong>Factorization</strong>, where the transformation from original SVM to FM lies.</p>

<p><img src="http://i.imgur.com/bgOUxWh.png" alt="" />
<img src="http://i.imgur.com/eHhxEsb.png" alt="" /></p>

<h2 id="fm-vs-mf">FM V.S. MF</h2>

<ul>
  <li>FM: <script type="math/tex">y(x)=\sum w_{i}x_{i}+\sum\sum(v_{i}\cdot v_{j})x_{i}x_{j} +b</script></li>
  <li>MF: <script type="math/tex">y(x)=w_{u}+w_{i}+v_{u}\cdot v_{i} + b</script></li>
</ul>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2013-10-05T00:00:00+08:00"><a href="/peppypapers/2013/10/05/incorporating-domain-knowledge-into-LDA.html">October 05, 2013</a></time></span><span class="author vcard"><span class="fn"><a href="/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="/peppypapers/2013/10/05/incorporating-domain-knowledge-into-LDA.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="/peppypapers/2013/10/05/incorporating-domain-knowledge-into-LDA.html" rel="bookmark" title="Incorpating Domain Knowledge into LDA" itemprop="url">Incorpating Domain Knowledge into LDA</a></h1>
    
  </header>
  <div class="entry-content">
    <p>Recent years, there has been emerging research on knowledge-based models and methods. Researchers have tried in various ways to express/embed/structure the knowledge and then incorporating them into some existing models, among which is LDA (Latent Dirichlet Allocation).</p>

<p>For further detailed about LDA, please investigate through [Blei el al., 2003].  The basic idea and foundation of LDA is handling <strong>word co-occurrence</strong> pattern to discover the latent semantic meaning. The simple model has limited resolution to deeper latent sementics and thus the variations of LDA are bursting. One focus to expand LDA is how to incorporating more <strong>prior knowledge</strong> into it.</p>

<h2 id="types-of-prior-knowledge">Types of Prior Knowledge</h2>

<p>Basically, all types of knowledge incorporation is to change the prior distribution of Dirichlet setting in LDA.</p>

<p>a.	在传统 LDA 里，有两组先验，一种是文档~主题的先验，来自于一个对称的<script type="math/tex">\mathcal Dir(\alpha)</script>；一种是主题~词汇的先验，来自于一个对称的<script type="math/tex">\mathcal Dir(\beta)</script> ——都是 symmetric Dirichlet Distribution。所以按理，可以把这两种先验分别改成不对称的——这样就加入了更多的 knowledge 信息。</p>

<p>b.	在 Rethinking LDA 里一文<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>中，结合两种先验与两种不同的先验设定方法，可以得到以下四种组合：</p>

<ul>
  <li>AA：文档~主题分布和主题~词汇分布都采用非对称；</li>
  <li>AS：文档~主题分布采用非对称的先验，而主题~词汇分布采用对称的先验；</li>
  <li>SA：文档~主题分布采用对称的先验，而主题~词汇采用非对称；</li>
  <li>SS：文档~主题分布和主题~词汇分布都采用对称的先验。</li>
</ul>

<p>他们的实验发现其中 AS 的方法可以更高地提高 LDA 对于文本建模的能力。</p>

<p>c.  典型的打破“对称”文档~主题分布先验（AS）的几个model，有很好理解的 Twitter-LDA，也有 Behavior-LDA。同时，supervised-LDA 也可以看做一个非结构化的打破先验的方式，变形后有 SeededLDA（在两个层次的先验都通过设计加入了不对称信息）。</p>

<p>d.	除了通过<strong>直接</strong>地改变概率分布来加入先验的方法，这几年来开始有越来越多的研究者想将结构化的先验知识加入 LDA 。这种结构化的先验，不再是简单的 prior distribution，更可以倾向于称为“knowledge”。这样的研究之所以盛行，一方面是长期以来的结构化知识库已有很多（且因为还要继续建立知识图谱等，结构化仍将是未来的趋势），另一方面形式语言（逻辑语言）的表示的研究一直都没有停止。这种结构化的引入 knowledge 的方法，本质也是通过打破先验设定的 symmetric Dirichlet Distribution。下文将重点总结这方面的工作。</p>

<h2 id="domain-dependent-model">Domain-dependent Model：</h2>
<ul>
  <li>
    <p>CIKM’13 里，Zhiyuan Chen（也在 Bing Liu那里）的一篇 Discovering Coherent Topics<sup id="fnref:3"><a href="#fn:3" class="footnote">2</a></sup> 里将 incorporating knowledge 的研究分成了 domain-dependent 的和 domain-independent：前者是 expert 知道（普通人不一定熟悉，需要 expert 来参与编辑）的知识而且有知识领域限制，后者是各领域通用的一些知识。</p>
  </li>
  <li>
    <p>同样是上述文章，提到了<sup id="fnref:2"><a href="#fn:2" class="footnote">3</a></sup>,<sup id="fnref:4"><a href="#fn:4" class="footnote">4</a></sup>,<sup id="fnref:5"><a href="#fn:5" class="footnote">5</a></sup>,<sup id="fnref:6"><a href="#fn:6" class="footnote">6</a></sup>,<sup id="fnref:7"><a href="#fn:7" class="footnote">7</a></sup> 的论文都是 domain-dependent knowledge-based 的。</p>
  </li>
  <li>
    <p>其中，Dirichlet Forest<sup id="fnref:2:1"><a href="#fn:2" class="footnote">3</a></sup> 和 Jerry Zhu 的 First-Order Logic<sup id="fnref:4:1"><a href="#fn:4" class="footnote">4</a></sup> 的形式化加入 domain-knowledge 的方法还是比较有代表性。前者是将领域内一定会一起出现（两个词的出现概率都很大或者都很小）的词和一定不能一起出现的词分别表示为 Must-Link 和 Cannot-Link，然后表示成树中的结点和结点之间的连接关系。但这个 Link 关系是可传递的，所以会导致“错误”的先验知识加入（CIKM’13 中提到了这点）。</p>
  </li>
</ul>

<h2 id="domain-independent-model">Domain-independent Model：</h2>

<ul>
  <li>
    <p>按照 Zhiyuan Chen 的说法，他们在 CIKM’13 里提出的 GK-LDA<sup id="fnref:3:1"><a href="#fn:3" class="footnote">2</a></sup> 应该是第一个 domain-independent model。所以这个部分只谈他们的那篇论文（GK-LDA 是 General Knowledge 的缩写，即 domain-independent 的 knowledge）。</p>
  </li>
  <li>
    <p>在这篇论文里，他们的假设是，<em>there is a vast amount of available in online dictionaries or other resources that can be exploited in a model to generate more coherent topic</em>. 而通过 extract，就可以把这样的 lexical knowledge 提取成 a general knowledge base.</p>
  </li>
  <li>
    <p>他们采取的知识表达结构是 lexical relationships on words. 简称 <strong>LR-sets</strong>。LR-sets 有很多种关系，比如同义词、反义词，这篇文章中重点讲的是 adjective-attribute 这种 relationship，e.g. (expensive-price).</p>
  </li>
  <li>
    <p>他们提出的 GK-LDA 依然是 一种 LDA 的变形，而且是基于他们组再之前的工作——IJCAI’13 的 Leveraging Multi-Domain Prior Knowledge<sup id="fnref:8"><a href="#fn:8" class="footnote">8</a></sup> 里的 MDK-LDA。</p>
  </li>
</ul>

<h2 id="mdk-lda-b--mdk-lda--gk-lda">从 MDK-LDA (b) 到 MDK-LDA 到 GK-LDA：</h2>

<ul>
  <li>
    <p>主要总结 Zhiyuan Chen 的两篇工作，之前提过的 MDK-LDA 和 GK-LDA。</p>
  </li>
  <li>
    <p>MDK-LDA 是 multi-domain knowledge 的缩写，从思想上来看是一种 Transfer Learning 的想法，prior from other domain can help topic model in new domain.</p>
  </li>
  <li>
    <p>所谓的 multi-domain 可以通过 <strong>s-set</strong> 表示，比如 “light” has 2 s-set {light, heavy, weight} 和 {light, bright, luminanee}，表示出了 light 的两个词义。那么这个工作就是去 leverage 这个 s-sets。</p>
  </li>
  <li>
    <p>他们在 IJCAI’13 的那篇里<sup id="fnref:8:1"><a href="#fn:8" class="footnote">8</a></sup> 主要分析了 之前的 domain-knowledge 会遇到的两个大问题，MDK-LDA 解决了其中一个 adverse effect 的问题，而 GK-LDA 两个都解决了（还有一个是错误先验知识带来的问题）。MDK-LDA 解决的主要在 LDA 问题里，如何使得一些少见的词但是确实是同一个 set 里的词的低频不会影响 topic modeling 的学习（不仅仅用 TF-IDF 消除影响），那么他们认为 <em>the words in an s-set share a similar semantic meaning in the model should redistribute the probability masses over words in the s-set to ensure that they have similar probability under the same topic</em>. 这个思想使得他们在 MDK-LDA(basic) 之上加入了 GPU <sup id="fnref:9"><a href="#fn:9" class="footnote">9</a></sup>：像抽出小球再放回同颜色的球的思想一样，去改变同一个 s-set 里的 word 的dist.</p>
  </li>
  <li>
    <p>在 MDK-LDA 之上，解决第二个问题的就是 GK-LDA，也就是在 CIKM’13 里的那篇<sup id="fnref:3:2"><a href="#fn:3" class="footnote">2</a></sup>。MDK-LDA 没法避免当我们的先验 s-set 是错误的（这也是其他许多 domain-dependent model 的问题，必须保证我们的先验知识都是正确的）对 performance 的影响。 GK-LDA 加入了一个 word correlation matrix 的计算 和 加入一个 threshold，减少了 wrong LR-set 的的影响。</p>
  </li>
  <li>
    <p>其中加入 GPU 的思想，和 CRP 中如何改变人坐在具体某个餐桌的概率的思想是一致的（只是一个模型的不同解释）。</p>
  </li>
</ul>

<h2 id="footnotes">Footnotes</h2>

<ul>
  <li>
    <p>Transfer Learning 和 Active Learning、Online Learning 等等都有关系。这部分内容还没有系统学习过，之前一篇<a href="http://yanran.li/2013/07/covariate-shift-correction/">文章</a>也有提到这里的一个小坑。</p>
  </li>
  <li>
    <p>GPU<sup id="fnref:9:1"><a href="#fn:9" class="footnote">9</a></sup>，是 Generalized Polya Urn 的简称。搞懂 LDA 必须先学习的模型。将这个过程generalized, 可以推向Polya Urn’s Process。Polya Urn’s Model 是比较直观的理解 Dirichlet Process 的一种解释模型。模型中抽出球再放回就是对当前的多项分布进行抽样（同时不改变该分布），又放回一个同样的球就是依当前多项分布产生新的多项分布。假设从<script type="math/tex">\mathcal Dir(\alpha, K)</script>中抽样，那么新产生的多项分布共有 K 个，其概率质量与当前多项分布成比例。K 个新产生的多项分布的加权平均与原多项分布是同分布的。而在之前的 CIKM’13 论文<sup id="fnref:3:3"><a href="#fn:3" class="footnote">2</a></sup>中就是通过改变每次放回的“球”（LR-set 里同一个 set 的词）的“颜色”和数量来改变 prior knowledge 的。这种思想感觉还是很赞的。</p>
  </li>
</ul>

<h2 id="reference">Reference</h2>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Hanna Wallach, David Mimno and Andrew McCallum. Rethinking LDA: Why Priors Matter. NIPS, 2009, Vancouver, BC. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun Hsu, Malu Castellanos, and Riddhiman Ghosh. Discovering Coherent Topics using General Knowledge. Proceedings of the ACM Conference of Information and Knowledge Management (CIKM’13). October 27 - November1, Burlingame, CA, USA. <a href="#fnref:3" class="reversefootnote">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote">&#8617;<sup>2</sup></a> <a href="#fnref:3:2" class="reversefootnote">&#8617;<sup>3</sup></a> <a href="#fnref:3:3" class="reversefootnote">&#8617;<sup>4</sup></a></p>
    </li>
    <li id="fn:2">
      <p>Andrzejewski, D., Zhu, X. and Craven, M. 2009. Incorporating domain knowledge into topic modeling via Dirichlet Forest priors. ICML, 25–32. <a href="#fnref:2" class="reversefootnote">&#8617;</a> <a href="#fnref:2:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:4">
      <p>Andrzejewski, D., Zhu, X., Craven, M. and Recht, B. 2011. A framework for incorporating general domain knowledge into latent Dirichlet allocation using first-order logic. IJCAI, 1171–1177. <a href="#fnref:4" class="reversefootnote">&#8617;</a> <a href="#fnref:4:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:5">
      <p>Burns, N., Bi, Y., Wang, H. and Anderson, T. 2012. Extended Twofold-LDA Model for Two Aspects in One Sentence. Advances in Computational Intelligence. Springer Berlin Heidelberg. 265–275. <a href="#fnref:5" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>Jagarlamudi, J., III, H.D. and Udupa, R. 2012. Incorporating Lexical Priors into Topic Models. EACL, 204–213 <a href="#fnref:6" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>Mukherjee, A. and Liu, B. 2012. Aspect Extraction through SemiSupervised Modeling. ACL, 339–348. <a href="#fnref:7" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:8">
      <p>Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun Hsu, Malu Castellanos, and Riddhiman Ghosh. Leveraging Multi-Domain Prior Knowledge in Topic Models. Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI’13). August 3-9, 2013, Beijing, China. <a href="#fnref:8" class="reversefootnote">&#8617;</a> <a href="#fnref:8:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:9">
      <p>David Mimno, Hanna Wallach, Edmund Talley, Miriam Leenders, Andrew McCallum. Optimizing Semantic Coherence in Topic Models. EMNLP (2011). <a href="#fnref:9" class="reversefootnote">&#8617;</a> <a href="#fnref:9:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
  </ol>
</div>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2013-10-04T00:00:00+08:00"><a href="/r/2013/10/04/loading-big-data-in-R.html">October 04, 2013</a></time></span><span class="author vcard"><span class="fn"><a href="/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="/r/2013/10/04/loading-big-data-in-R.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="/r/2013/10/04/loading-big-data-in-R.html" rel="bookmark" title="Loading Big Data in R" itemprop="url">Loading Big Data in R</a></h1>
    
  </header>
  <div class="entry-content">
    <p>Although parallel techniques in R has been prevailing, I will only focus on Loading the complete data into RAM in R, that is to say, no Hadoop or similar. What other more I <em>won’t</em> mention in this post is about <a href="http://www.r-bloggers.com/big-data-analysis-for-free-in-r-or-how-i-learned-to-load-manipulate-and-save-data-using-the-ff-package/">manipulating and saving big data in R</a>, and parallel computing.</p>

<p>Just start with different implementations:</p>

<ul>
  <li>
    <p>load <strong>csv</strong> file and using <strong>ff</strong> package (Rtools)</p>

    <pre><code>  bigdata &lt;- read.csv.ffdf(file = ”bigdata.csv”, first.rows=5000, colClasses = NA)
</code></pre>

    <p><em>Notice</em> that ff package should be in Rtools on Windows.</p>
  </li>
  <li>
    <p>using <strong>sqldf()</strong> from <strong>SQLite</strong></p>

    <p>this is a method from <a href="http://stackoverflow.com/a/1820610/1849063">StackOverflow</a>: using sqldf() to import the data into SQLite as a staging area, and then sucking it from SQLite into R</p>

    <pre><code>  library(sqldf)
  f &lt;- file("bigdf.csv")
  system.time(bigdf &lt;- sqldf("select * from f", dbname = tempfile(), file.format = list(header = T, row.names = F)))
</code></pre>
  </li>
  <li>
    <p>magic <strong>data.table</strong> and <strong>fread</strong></p>

    <p>it includes data.frame, but some of the syntax is different. Luckily, the <a href="http://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.pdf">documentation</a> (and the FAQ) are excellent.</p>

    <p>Read csv-files with the fread function instead of read.csv (read.table). It is faster in reading a file in table format and gives you feedback on progress.</p>

    <p><em>Notice</em> that fread() cannot directly read gzipped files and it comes with a big warning sign “not for production use yet”. One trick it uses is to read the first, middle, and last 5 rows to determine column types.</p>
  </li>
  <li>
    <p>optimized <strong>read.table()* with **colClasses</strong></p>

    <p>This option takes a vector whose length is equal to the number of columns in year table. Specifying this option instead of using the default can make ‘read.table’ run MUCH faster, often twice as fast. In order to use this option, you have to know the of each column in your data frame. - See more at <a href="http://simplystatistics.tumblr.com/post/11142408176/r-workshop-reading-in-large-data-frames#sthash.IpNe4GfP.dpuf">hear</a>.</p>

    <pre><code>  read.table("test.csv",header=TRUE,sep=",",quote="",  
                    stringsAsFactors=FALSE,comment.char="",nrows=n,                   
                    colClasses=c("integer","integer","numeric",                        
                                 "character","numeric","integer"))
</code></pre>
  </li>
  <li>
    <p>load a <strong>portion</strong> using <strong>nrows</strong></p>

    <p>Also you can read in only a portion of your file, to get a feel of the dataset.</p>

    <pre><code>  data_first_100 &lt;- read.table("file", header=T, sep="\t", stringsAsFactors=F, nrows=100)
</code></pre>
  </li>
  <li>
    <p>in summary</p>

    <p><a href="http://stackoverflow.com/a/15058684/1849063">Here</a> is a great comparison summary for the method above with their system time. I just copy the summary table below:</p>

    <pre><code>  ##    user  system elapsed  Method
  ##   24.71    0.15   25.42  read.csv (first time)
  ##   17.85    0.07   17.98  read.csv (second time)
  ##   10.20    0.03   10.32  Optimized read.table
  ##    3.12    0.01    3.22  fread
  ##   12.49    0.09   12.69  sqldf
  ##   10.21    0.47   10.73  sqldf on SO
  ##   10.85    0.10   10.99  ffdf
</code></pre>
  </li>
</ul>

<p>See more in <a href="http://www.theodi.org/blog/fig-data-11-tips-how-handle-big-data-r-and-1-bad-pun">11 Tips on How to Handle Big Data in </a>.</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2013-09-27T00:00:00+08:00"><a href="/naturallanguageprocessing/2013/09/27/from-word-embedding-to-language-model_1.html">September 27, 2013</a></time></span><span class="author vcard"><span class="fn"><a href="/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="/naturallanguageprocessing/2013/09/27/from-word-embedding-to-language-model_1.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="/naturallanguageprocessing/2013/09/27/from-word-embedding-to-language-model_1.html" rel="bookmark" title="From Word Embedding to Language Model(1)" itemprop="url">From Word Embedding to Language Model(1)</a></h1>
    
  </header>
  <div class="entry-content">
    <h2 id="deep-learning-basement">Deep Learning Basement</h2>

<p>There are some basic important concepts in deep learning.</p>

<h3 id="backpropagation-network">Backpropagation Network</h3>

<p>BP Network is a special case of <strong>Feedforward networks</strong>, constituted by <strong>nonlinear continuous transformation</strong> units, and adjusted by <strong>error back propagation algorithm</strong>. The BP algorithm can be divided into two phases: propagation and weight update.</p>

<p>Also, some techniques are used to improve BP algorithm. For instance, gentic algorithm (supervised) and RBM (unsupervised), Auto-decoder (Deep Learning) can be in order to search for good weights before training.</p>

<h3 id="distributed-representations">Distributed Representations</h3>

<p>In contrast to the “atomic” or “localist” representations employed in traditional cognitive science, a distributed representation is one in which <strong>“each entity is represented by a pattern of activity distributed over many computing elements, and each computing element is involved in representing many different entities.”</strong></p>

<p>Yoshua Bengio gave a vivid analogy between “local” and “distributed” in his given talk, <em>Learning to Represent Semantics</em>.</p>

<p><img src="http://i.imgur.com/b8sEQFd.png" alt="" /></p>

<h2 id="deep-learning-motivation-for-semantics">Deep Learning Motivation for Semantics</h2>

<p>In Language Model, given a probability for a longer word sequence, 
<script type="math/tex">P(w_{1},...,w_{l})=\prod_{t}P(w_{t}|w{t-1},...,w_{t-n+1})</script>, 
we then predict P(next word|context). And in traditional n-gram Language Model, we use counts and smoothing to calculate the conditional probability.</p>

<p>However, the traditional n-gram LM fails with <em>Curse of dimensionality</em>: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training.</p>

<p>For example, the training sentence:</p>

<pre><code>The cat is walking in the bedroom.
</code></pre>

<p>The Test sentence:</p>

<pre><code>A dog was running in a room.
</code></pre>

<p>Under the sparsity/curse of dim. problem, we seek for a <strong>similar representations for semantically similar phrases</strong>.</p>

<h3 id="word-embedding-for-representing-words">Word Embedding for Representing Words</h3>

<p><strong>Word Embedding</strong> gives a low dimension (usually 50) distributed representation (Hinton, 1986) for each word. Thus similar words have similar representations. This distributed continuous-valued vector for each word can be learned from raw text (Colobert &amp; Weston, 2008).</p>

<h3 id="composing-words-with-nn">Composing Words with NN</h3>

<p>The Word Embedding is only the representation for each word, we should learn how to compose words into phrases and semantic relations. And Theorems on advantage of depth  (Hastad et al 86 &amp; 91, Bengio et al 2007, Bengio &amp;
Delalleau 2011, Braverman 2011) proves that deep architectures are more expressive and sharing components exponentially strengthens the advantage.</p>

<h2 id="neural-network-language-model">Neural Network Language Model</h2>

<p>Training the word embedding and the neural network at the same time, there are two types of system: Neural network language model and the others. The former type consists of (1)input: context; (2)output: distribution of next word, <script type="math/tex">\vert V\vert</script> nodes. And the latter type (1)input: entire sequence; (2)output: score, 1 node.</p>

<h3 id="probabilistic-neural-language-model">Probabilistic Neural Language Model</h3>

<table>
  <tbody>
    <tr>
      <td>Back to the problem, predict P(next word</td>
      <td>context). To calcute the conditional probability:</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Traiditional N-gram Languge Model uses counts and smoothing.</li>
  <li>Probabilistic Neural Language Model uses word embedding and neural network.</li>
</ul>

<p>After building the language model, compute word vectors during this process:</p>

<script type="math/tex; mode=display">f(i,w_{t-1},...,w_{t-n+1})=g(i,C(w_{t-1}),...,C(w_{t-n+1}))</script>

<p><img src="http://i.imgur.com/gLGi5vU.png" alt="" /></p>

<p>This structure is Bengio’s groundbreaking work.
It has a linear projection layer, a nonlinear hidden layer and a softmax output layer. The sparse history h is projected into some continuous low-dimensional space, where similar histories get clustered. Moreover, the model is more robust: less parameters have to be estimated from the training data.</p>

<p>But the model has limitation below:</p>

<ul>
  <li>Complexity: <script type="math/tex">(n × m) × h + h × \vert V\vert</script></li>
  <li>New words fails</li>
  <li>Long term context ignored</li>
  <li>Lack priori knowledge, such as POS, semantic information (WordNet)</li>
</ul>

<h2 id="problems-remain">Problems Remain</h2>

<p>How to design a neural network? An art or a science? :P</p>

<h2 id="references">References</h2>

<p>Yoshua Bengio. Learning to Represent Semantics. Words2Actions Workshop, NAACL HLT 2012, Montreal</p>

<p>Hinton, G. E., McClelland, J. L., and Rumelhart, D. E. (1986). Distributed representations. In Rumelhart, D. E. and McClelland, J. L., editors, Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations, MIT Press, Cambridge, MA.</p>

<p>R. Collobert and J. Weston. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In International Conference on Machine Learning, ICML, 2008.</p>

<p>Morin and Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS 2005.</p>

<p>Mnih and Hinton. A Scalable Hierarchical Distributed Language Model. NIPS 2008.</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2013-07-14T00:00:00+08:00"><a href="/machinelearning/2013/07/14/basics-for-regularization.html">July 14, 2013</a></time></span><span class="author vcard"><span class="fn"><a href="/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="/machinelearning/2013/07/14/basics-for-regularization.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="/machinelearning/2013/07/14/basics-for-regularization.html" rel="bookmark" title="Basics for Regularization" itemprop="url">Basics for Regularization</a></h1>
    
  </header>
  <div class="entry-content">
    <h2 id="overfittingregularization">为什么会有overfitting，为什么regularization有效</h2>
<p>对于某一个给定的 <script type="math/tex">\mathcal{F}</script> ，根据大数定理，当 n 趋向于无穷时，经验风险泛函是（依概率）收敛于风险泛函的。
但是机器学习是在一个给定的函数空间 <script type="math/tex">\mathcal{H}</script> 中搜索一个最优的（使得风险泛函最小的）函数，因此为了保证这个搜索过程是合理的，需要保证对于整个空间 <script type="math/tex">\mathcal{H}</script> 中的所有函数能一致收敛。
于是，当n趋于无穷时表现好并不是就代表 n 有限的时候同样好，即在 n 有限时，直接去最小化经验风险泛函并不一定是最优的，会造成 overfitting 等问题。所以，要分析在n 有限时，给出一个 <script type="math/tex">\mathcal{R_{P_n}}</script> 和 <script type="math/tex">\mathcal{R_P}</script> 之间的差别的 bound ，这个 bound 不仅依赖于 n 的大小，还依赖于我们所用的目标函数空间的“大小”——比如用 VC 维之类的东西来刻画。因此，在最小化经验风险泛函的同时，通过正则化的方法同时去最小化目标函数空间的“大小”——即“结构风险最小化”。</p>

<h2 id="regularization">Regularization带来什么</h2>
<p>这部分是看《Learning From Data》的slides里讲到的。</p>

<p>Regularization 其实是 constrain 了搜索空间，比如 constrain 了最小二乘的 weight 大小。
带来的结果是 bias 有可能增加（side-effect），但 variance 降低（这是期望带来的）。</p>

<h2 id="l0-l1-l2">L0, L1, L2都带来什么</h2>
<p>L0应该就是SRM，结构风险最小化。据说可以用来筛掉指数级别的不相关feature，但是不可求解。</p>

<p>(1) <script type="math/tex">\min_{w} 1/n \sum_{i=1}^{n}l(y,f_{w}(x))+\lambda count\left \{ {w_{j}\neq 0} \right \}</script></p>

<p>L1也是NP-hard问题（本质因为可以和L0对等），是个菱形，最初用于所谓的compressed sensing。</p>

<p>(2) <script type="math/tex">\min_{w} 1/n \sum_{i=1}^{n}l(y,f_{w}(x))+\lambda \left \| w \right \|_{1}</script></p>

<p>L2是个球形，保持旋转不变性。</p>

<p>(3) <script type="math/tex">\min_{w} 1/n \sum_{i=1}^{n}l(y,f_{w}(x))+\lambda \left \| w \right \|_{2}</script></p>

<p>L1和L2的话，L1可以作为特征选择（本质也是因为对等到L0，而L0中是非零分量个数的正则），但是比较难求解（难是说需要用优化算法求解，比如Bregman Iteration）；L2求解方便，不会引入很高的复杂度。</p>

<p>关于L1和L0优化的等价问题可以参考 Candes，Donoho 等人的 Compressed sensing 理论。</p>

<p>当然还有什么L1/2之类的，是 non-convex 的。先不说了。</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2013-07-03T00:00:00+08:00"><a href="/statistics/2013/07/03/covariate-shift-correction.html">July 03, 2013</a></time></span><span class="author vcard"><span class="fn"><a href="/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="/statistics/2013/07/03/covariate-shift-correction.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="/statistics/2013/07/03/covariate-shift-correction.html" rel="bookmark" title="Covariate Shift Correction" itemprop="url">Covariate Shift Correction</a></h1>
    
  </header>
  <div class="entry-content">
    <p><a href="http://weibo.com/bit9">江申</a>在《DSP中的算法初探》中提到了一个 Covariate Shift 的问题，第一次听说，查了一下，发现是一个非常重要的问题。</p>

<p>在 DSP 中，这个问题叫做 Bid Adjustment:</p>

<pre><code>在线上生产环境进行实际竞价时，通常需要对竞价模型的参数做调整。
</code></pre>

<p>原因：1）线上的数据分布与线下用的训练数据的分布不一样，需要对参数做调整；
2）线上的环境是动态变化的，得让参数也随之变化。</p>

<p><strong>Covariate Shift</strong> 就是说， training and test data were so different，<strong>我们在 training 过程中 sampling 假设的 distribution 和实际真实的 distribution 差异太大了</strong>导致我们最后的training 是 waste。</p>

<p>解决办法是通过两步：</p>

<ul>
  <li>Step 1: 得到真实分布 q 和假设的分布 p 之间的 ratio</li>
  <li>Step 2: reweight training set</li>
</ul>

<p>Step 1就需要考虑如何去衡量两个分布之间的差异。直观的方法是：训练一个 LR 模型，数据为“训练+待预测”数据，Label 为是否属于训练集。分得准，差异大。分不准，差异小。
理论上这里 用 任意learning方法出来的 classifier 都是可以的（见 paper: <a href="http://jmlr.org/papers/volume10/bickel09a/bickel09a.pdf最后的conclusion">Discriminative Learning Under Covariate Shift</a> conclusion 的部分）。</p>

<p>特别的，如果用LR的话，简单的推导见 Alex Smola 的一篇 <a href="http://blog.smola.org/post/4110255196/real-simple-covariate-shift-correction">blog</a>。</p>

<ul>
  <li>Step 2 学到了这个 ratio 就可以做 reweight。</li>
</ul>

<p>re-weight each instance by the ratio of probabilities that it would have been drawn from the correct distribution, that is, we need to reweight things by p(xi)q(xi). This is the ratio of how frequently the instances would have occurred in the correct set vs. how frequently it occurred with the sampling distribution q.</p>

<p>很多 Transfer Learning 的方法都和这个类似。</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2013-06-27T00:00:00+08:00"><a href="/r/2013/06/27/loading-data-in-R.html">June 27, 2013</a></time></span><span class="author vcard"><span class="fn"><a href="/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="/r/2013/06/27/loading-data-in-R.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="/r/2013/06/27/loading-data-in-R.html" rel="bookmark" title="Loading Data in R" itemprop="url">Loading Data in R</a></h1>
    
  </header>
  <div class="entry-content">
    <p>There are several types of problems one may meet when loading data in R. I solved some of them and taken down the notes below.</p>

<h4 id="tough-garbage-cn-characters-in-r">tough garbage CN characters in R</h4>
<p>中文乱码的问题在很多情况下都遇到了。内因是R是用本地码（通常是GBK）来解释unicode。 
目测整体解决办法有几种：</p>

<ul>
  <li>Encodings</li>
</ul>

<p>这个办法我只成功解决过一次。</p>

<pre><code>source(file,encoding="utf-8") 
</code></pre>

<ul>
  <li>
    <p>改R的环境</p>

    <p>很奇怪的是在英文环境下都反而有时候不乱码。</p>
  </li>
  <li>
    <p>操作系统的系统编码问题</p>

    <p>Windows是 gbk 编码，且不可改！（所以只能 Encodings 改了）；Linux 是 utf-8 。可以用 sessioninfo() 来查看 locale 的编码，然后改掉。一般有时候比如 mysql 也乱码的时候这个方法很好用，应该是个通用性很高的方法。</p>

    <p><em>Windows</em> 
  一般是gbk的编码，读取utf-8的文件时，需要声明读取编码就OK了。</p>

    <pre><code>  source(file,encoding="utf-8") 
</code></pre>

    <p><em>Linux</em>的情况复杂一些 
  * locale要设置成zh_CN 
  * 要安装中文字符集，或者从window下复制过去 
  * R读取，统一用utf-8的。</p>

    <p>最复杂的情况是<em>DB连接</em> 
  * 有时候DB的字符集是gb2312, gbk, utf8等 
  * 在DB读取的时候，DBI包，要设置DB的字符编码 
  * 当把数据读到R中时，要跟R的环境的编码要统一 
  * linux/win两套环境，编码部分要是区别写的。</p>

    <p><a href="http://f.dataguru.cn/thread-20496-1-1.html">Ref</a></p>
  </li>
  <li>
    <p>强大的iconv()</p>

    <p><a href="http://stat.ethz.ch/R-manual/R-patched/library/base/html/iconv.html"><em>Usage</em></a></p>

    <pre><code>  iconv(x, from = "", to = "", sub = NA, mark = TRUE) 
  iconvlist()  
</code></pre>

    <p>除此以外还可以用于除掉一些乱码，比如 Removing non-ASCII characters.  <a href="http://stackoverflow.com/questions/9934856/removing-non-ascii-characters-from-data-files">Ref</a></p>
  </li>
  <li>
    <p>强大的iconv()也失效时</p>

    <ul>
      <li>
        <p>更多更好的去理解网页编码 <a href="http://yishuo.org/r/2012/09/13/junk-code-again.html">Ref</a></p>

        <pre><code>  url= htmlParse(url,encoding="UTF-8")  
</code></pre>
      </li>
      <li>
        <p>embedded null characters (‘\0’) in strings</p>
      </li>
    </ul>

    <p>这个似乎也是个 devils 在 inferno 的书里有写，下次再开坑吧。 <a href="http://biostatmatt.com/archives/456">Ref</a></p>
  </li>
</ul>

<h4 id="missing-values">missing values</h4>

<p>大概是 missing value 要仔细处理。</p>

<p>和 missing value 有关的大概有4件事：</p>

<ul>
  <li>如何填充 missing value</li>
  <li>misquote 等等会引起 missing value</li>
  <li>whitespace 可能丧失</li>
  <li>
    <p>extraneous fields 用 fill 解决或者用 count.fields 诊断</p>

    <pre><code>  x &lt;- count.fields("UserProfile.tsv", sep = '\t') 
  table(x) 
  which(x != legal.length) // check where the illegal lines are 
	
	
  userlist &lt;- read.table("UserProfile.tsv", sep = '\t', header = FALSE, stringAsFactors = FALSE, fill = TRUE) // "file" matters. 
</code></pre>
  </li>
</ul>

<p>其中填充 missing value 涉及到 na.strings()。这里牵扯到如果一个 string value 真的是 NA，要注意加quote。 <a href="https://science.nature.nps.gov/im/datamgmt/statistics/r/fundamentals/manipulation.cfm">Ref</a></p>

<p>再之， 对 NA 的问题又牵扯出 <a href="http://www.ats.ucla.edu/stat/r/faq/missing.htm">na.action</a>.</p>

<h4 id="group-to-summary">group to summary</h4>

<ul>
  <li>The ddply() function. It is the easiest to use, though it requires the plyr package. This is probably what you want to use.</li>
  <li>The summarizeBy() function. It is easier to use, though it requires the doBy package.</li>
  <li>The aggregate() function. It is more difficult to use but is included in the base install of R.</li>
</ul>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    <div class="entry-meta"><span class="entry-date date published updated"><time datetime="2011-07-01T00:00:00+08:00"><a href="/bookreviews/2011/07/01/how-to-train-critical-thinking.html">July 01, 2011</a></time></span><span class="author vcard"><span class="fn"><a href="/about/" title="About Yanran Li">Yanran Li</a></span></span>&nbsp; &bull; &nbsp;<span class="entry-comments"><a href="/bookreviews/2011/07/01/how-to-train-critical-thinking.html#disqus_thread">Comment</a></span></div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="/bookreviews/2011/07/01/how-to-train-critical-thinking.html" rel="bookmark" title="如何训练批判性思维" itemprop="url">如何训练批判性思维</a></h1>
    
  </header>
  <div class="entry-content">
    <p>《学会提问——批判性思维指南》 
　　 
<img src="http://img3.douban.com/lpic/s1509100.jpg" alt="" /></p>

<h2 id="section">步骤</h2>

<p>1、 “谁关心这个话题” ：你的时间是宝贵的，在你花费时间对某个问题进行批判性评价前，先问问who cares。</p>

<p>2、找出结论</p>

<p>　　	在找出结论之前我们无法做出批判性评价！</p>

<p>　　	结论就是演讲者或作者希望你接受的信息。</p>

<p>　　	没有支持的言论仅仅是一些观点而非结论。</p>

<p>3、确定理由是批判性思维的重要步骤</p>

<p>　　
	要问“为什么”：要确定你是否发现了一条理由，最好的方式是你尽量扮演作者的角色。</p>

<p>　　	结论依赖于理由的价值。</p>

<p>4、寻找干扰性原因</p>

<p>　　	当你有充足的理由相信作者或演说者对某件事的因果解释的证据时，你就需要寻找干扰性原因。</p>

<p>　　	一旦你发现一个因果说明，一定要警惕干扰性原因存在的可能性。</p>

<p>5、被遗漏的信息 
　　
	我们要特别强调下面这种被遗漏的信息，因为这种信息非常重要，但却常常被忽略，这就是：作者所提倡的行为可能产生的消极作用。 
　　</p>

<h2 id="section-1">概念类</h2>

<h3 id="section-2">两种思维——海绵式思维、淘金式思维</h3>

<p>1、海绵式思维：吸收、获得</p>

<p>　　优点1：吸收的越多，越能理解它的复杂性。</p>

<p>　　优点2：相对被动，主要的心理加工就是注意和记忆。</p>

<p>　　缺点1：始终相信其最后接收的信息</p>

<p>2、淘金式思维：与知识积极的互动</p>

<p>　　优点1：评判所见所闻的价值</p>

<p>　　优点2：回报巨大</p>

<p>　　缺点1：艰辛、具有挑战性 
　　</p>

<h3 id="section-3">强、弱批判性思维</h3>

<p>1、弱——是用批判性思维维护你自己已有的观点</p>

<p>2、强——是运用相同的技能来评估所有的观点和信念，特别是评估自己的观点和信念</p>

<h3 id="section-4">论题的种类</h3>

<p>1、描述性论题——针对有关过去、现在、未来的描述是否正确提出的问题。</p>

<p>2、说明性论题——针对我们应当怎样做及对与错、好与提出的问题。</p>

<h3 id="section-5">文章基本要素：论题、结论、理由</h3>

<p>理由包括信念、证据、比喻、类推以及其他用来支持或证明这些结论的陈述。</p>

<h3 id="section-6">两种假设</h3>
<p>　　
1、描述性假设——关于世界是什么样子的观念</p>

<p>2、说明性或价值观假设——关于世界应当怎样的观念</p>

<h3 id="section-7">三个通常的谬误</h3>

<p>1、提供了错误或不正确假设的推理</p>

<p>2、通过使信息看起来与结论相关而实际上不相关来转移我们的视线</p>

<p>3、需要使用已经被证实为真的结论来为结论提供支持</p>

<h3 id="section-8">书摘</h3>

<blockquote>

  <p>　　1、	人类的行为是如此的矛盾和复杂，因此我们对人类行为有问题的最好答案在本质上就带有不确定性。    <br />
　　2、	旧答案和新答案之间的相互影响是我们成长的基础。     <br />
　　3、	决定是否赞同某个观点的根本一步就是确定关键词或关键句的准确含义。 
　　4、	政治性的语言常常附带有感情色彩，并有歧义。      <br />
　　5、	请记住：你的听众不能长时间地专注于你说的话。如果你让某个听众感到困惑，那么你可能很快就会失去他；如果你不能重新吸引他的注意，你就是一个失败的传达者。 
　　6、	在所有论证中，都存在一些作者所认同的思想，而这类思想的典型特征就是作者没有对它们进行清晰的陈述。 
　　7、	在推理的结构中，这些思想是隐形的重要环节，是将全部论证整合在一起的黏合剂。 
　　8、	事实上，只有当推理中加入价值观假设时，作者的理由才能在逻辑上支持结论。 
　　9、	同一个价值观对不同的人来说，强烈程度是不同的。在回答一个说明性问题时，价值观的这种相对强度就会导致你得出与别人不同的答案。 
　　10、	我们仅仅在某一个点上保持自己的价值观倾向。 
　　11、	即使人们具有相同的价值观假设，也有可能得出不同的结论，因为人们对于每种结果产生的可能性及其影响程度有着不一致的看法。 
　　12、	你要记住，由于诸多原因，权威的意见常常是错误的。 
　　13、	记住：两事件相关并不能证明它们之间有因果关系！ 
　　14、	另外，“基本归因错误”是一种常见的偏见，即在解释他人的行为时，我们会过高地估计个人倾向性的作用，而降低了环境因素的作用。 
　　15、	推理往往是不完整的。因此，如果你一遇到信息缺失就自动声明你无法得出结论，那么，你就永远无法形成任何观点。</p>
</blockquote>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->


<div class="pagination">
  
    
      <a href="" class="btn">Previous</a>
    
  
  <ul class="inline-list">
    <li>
      
        <a href="">1</a>
      
    </li>
    
      <li>
        
          <span class="current-page">2</span>
        
      </li>
    
  </ul>
  
    Next
  
</div><!-- /.pagination -->
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2015 Yanran Li. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/hpstr/">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="/assets/js/scripts.min.js"></script>

          

</body>
</html>